{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Data is [fetched from the Api](api_.py) socket provided, https://services.nvd.nist.gov/rest/json/cves/1.0/. \n",
    "\n",
    "A local [MongoDB is established](mongo_.py) and the data is upserted. \n",
    "\n",
    "With [mongo queries](pddf.py) the data is unraveled, split into two, for CVSSv2 and CVSSv3. It is then returned as pandas' dataframes.\n",
    "\n",
    "The `Run()` class will perform all this with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from run import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish and Fill MongoDB\n",
    "Connects to the API and builds succesive queries by default starting in 2014 waiting 1s between each query.\n",
    "\n",
    "Upserts to a local MongoDB established with default settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-01-01T00:00:00:000%20UTC-00:00&pubEndDate=2014-05-01T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2205\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-01-01T00:00:00:000%20UTC-00:00&pubEndDate=2014-05-01T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2205 /2205 \n",
      "0 documents added to  collection,  2205 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-05-01T00:00:00:000%20UTC-00:00&pubEndDate=2014-08-29T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2018\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-05-01T00:00:00:000%20UTC-00:00&pubEndDate=2014-08-29T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2018 /2018 \n",
      "0 documents added to  collection,  2018 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-08-29T00:00:00:000%20UTC-00:00&pubEndDate=2014-12-27T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /3638\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-08-29T00:00:00:000%20UTC-00:00&pubEndDate=2014-12-27T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 3638 /3638 \n",
      "3638 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-12-27T00:00:00:000%20UTC-00:00&pubEndDate=2015-04-26T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2223\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2014-12-27T00:00:00:000%20UTC-00:00&pubEndDate=2015-04-26T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2223 /2223 \n",
      "2223 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2015-04-26T00:00:00:000%20UTC-00:00&pubEndDate=2015-08-24T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2056\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2015-04-26T00:00:00:000%20UTC-00:00&pubEndDate=2015-08-24T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2056 /2056 \n",
      "2056 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2015-08-24T00:00:00:000%20UTC-00:00&pubEndDate=2015-12-22T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2157\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2015-08-24T00:00:00:000%20UTC-00:00&pubEndDate=2015-12-22T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2157 /2157 \n",
      "2157 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2015-12-22T00:00:00:000%20UTC-00:00&pubEndDate=2016-04-20T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "harvested : 1914 /1914 \n",
      "1914 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-04-20T00:00:00:000%20UTC-00:00&pubEndDate=2016-08-18T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2390\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-04-20T00:00:00:000%20UTC-00:00&pubEndDate=2016-08-18T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2390 /2390 \n",
      "2390 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-08-18T00:00:00:000%20UTC-00:00&pubEndDate=2016-12-16T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /2057\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-08-18T00:00:00:000%20UTC-00:00&pubEndDate=2016-12-16T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "harvested : 2057 /2057 \n",
      "2057 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-12-16T00:00:00:000%20UTC-00:00&pubEndDate=2017-04-15T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /4500\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-12-16T00:00:00:000%20UTC-00:00&pubEndDate=2017-04-15T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "starting at: 4000 /4500\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2016-12-16T00:00:00:000%20UTC-00:00&pubEndDate=2017-04-15T00:00:00:000%20UTC-00:00&startIndex=4000&resultsPerPage=2000\n",
      "harvested : 4500 /4500 \n",
      "4500 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-04-15T00:00:00:000%20UTC-00:00&pubEndDate=2017-08-13T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /4843\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-04-15T00:00:00:000%20UTC-00:00&pubEndDate=2017-08-13T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "starting at: 4000 /4843\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-04-15T00:00:00:000%20UTC-00:00&pubEndDate=2017-08-13T00:00:00:000%20UTC-00:00&startIndex=4000&resultsPerPage=2000\n",
      "harvested : 4843 /4843 \n",
      "4843 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-08-13T00:00:00:000%20UTC-00:00&pubEndDate=2017-12-11T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n",
      "starting at: 2000 /4751\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-08-13T00:00:00:000%20UTC-00:00&pubEndDate=2017-12-11T00:00:00:000%20UTC-00:00&startIndex=2000&resultsPerPage=2000\n",
      "starting at: 4000 /4751\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-08-13T00:00:00:000%20UTC-00:00&pubEndDate=2017-12-11T00:00:00:000%20UTC-00:00&startIndex=4000&resultsPerPage=2000\n",
      "harvested : 4751 /4751 \n",
      "4751 documents added to  collection,  0 already exist\n",
      "waiting 1s ...\n",
      "https://services.nvd.nist.gov/rest/json/cves/1.0/?pubStartDate=2017-12-11T00:00:00:000%20UTC-00:00&pubEndDate=2018-04-10T00:00:00:000%20UTC-00:00&resultsPerPage=2000\n"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#NOTE: will automatically start downloading and\n",
    "# try to connect to a default mongoDB client\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "Run(collection='t').fill_mongo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Explore the data to get a better understanding of the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Pandas' dataframes\n",
    "Two df's returned for CVSSv2 and v3 which are merged together using an outer join\n",
    "\n",
    "- Create a Data frame with one row per CVE id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = Run(collection= 't').fill_df()\n",
    "dfV2,dfV3 = dfs.dfV2,dfs.dfV3\n",
    "try:\n",
    "    df = pd.merge( dfV3, dfV2, 'outer', '_id',suffixes=['_V3', '_V2'])\n",
    "except:\n",
    "    print('''ERROR: Possibly no MongoDB loaded\\nCreating df from backupDB.csv''')\n",
    "    pd.read_csv('backupDB.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many CVEs have CVSSv3 metrics versus only CVSSv2 metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Total n of CVE's = {len(df)}\n",
    "with CVSSv3 = {len(df.vectorString_V3.dropna())}\n",
    "with CVSSv2 = {len(df.vectorString_V2.dropna())}\n",
    "with just CVSSv2 = {len((df[df['vectorString_V3'].isnull()])['vectorString_V2'].dropna())}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for unique values in each column to determine if categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if df[i].dtype == object and len(df[i].unique()) <10:\n",
    "        print (df[i].unique(), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attackVector` and `accessVector` are nominal categorical and need dummy variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δ_list = [['attackVector','AV_3'], ['accessVector', 'AV_2']]\n",
    "dum_add = lambda ele: pd.get_dummies(df[ele[0]],prefix=ele[1])\n",
    "\n",
    "frames = [dum_add(i) for i in Δ_list]\n",
    "\n",
    "df = df.drop([i[0] for i in Δ_list], axis=1)\n",
    "\n",
    "frames.append(df)\n",
    "\n",
    "df = pd.concat(frames,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the other columns are ordinal categorical(e.g. `LOW`, `MEDIUM`, `HIGH` ) or boolean and can be filled with a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = {'NONE': 0, 'LOW':1, 'MEDIUM':2,'HIGH':3, 'CRITICAL':4,\n",
    "'PARTIAL': 1, 'COMPLETE':2,\n",
    "'SINGLE' :1, 'MULTIPLE' :2,\n",
    "'UNCHANGED':0 ,'CHANGED':1,\n",
    "'REQUIRED':1,\n",
    "False:0, True:1\n",
    "}\n",
    "df.replace(cat_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating general index of all keys for manipulating columns and adds the new dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfV2_keys,dfV3_keys = dfs.dfV2.keys(),dfs.dfV3.keys()\n",
    "\n",
    "\n",
    "transform_idx = df.T.index\n",
    "\n",
    "for i in transform_idx:\n",
    "    #print(i[:4])\n",
    "    if i[:4] == 'AV_3':\n",
    "        dfV3_keys = dfV3_keys.append([[i]])\n",
    "    if i[:4] == 'AV_2':\n",
    "        dfV2_keys = dfV2_keys.append([[i]])\n",
    "\n",
    "dfV3_keys = dfV3_keys.drop('attackVector')\n",
    "dfV2_keys = dfV2_keys.drop('accessVector')\n",
    "\n",
    "setDf = dfV3_keys.union(dfV2_keys)#set(dfV3_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating an index of all columns that appear in both CVSSv2 and v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxPairs = []\n",
    "\n",
    "for i in setDf: \n",
    "    substr_i = transform_idx[transform_idx.str.startswith(i+'_')]\n",
    "    if len(substr_i) >0:\n",
    "        idxPairs.append(substr_i)\n",
    "    \n",
    "\n",
    "#idxPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df.filter(regex='version*').columns\n",
    "df[f] = df[f].astype(float)\n",
    "df = df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both CVSSv2 and CVSSv3 have the same set of impact metrics, i.e. Confidentiality, Integrity and Availability, however their values are slightly different. For example, CVSSv2 uses complete (C) to represent the highest level of impact, but CVSSv3 uses high (H) instead. Is it possible to directly map from CVSSv2 impact metric values to CVSSv3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['confidentialityImpact','integrityImpact','availabilityImpact']:\n",
    "    print(df.corr()[i+'_V2'][i+'_V3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the correleation matrix between versions of the suggested metrics is not = 1, it would NOT be a good idea to directly map them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "predict the CVSSv3 Scope metric for CVEs without a CVSSv3 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What type of learning problem is this? What is the target? \n",
    "\n",
    "This is a supervised classification problem with a single variable `scope` as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis  import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selects and combines the metrics of interest from previous indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cvssV3 = []\n",
    "cvssV2 = []\n",
    "dfV3_keys_cp = dfV3_keys.drop(['_id','vectorString'])\n",
    "dfV2_keys_cp = dfV2_keys.drop(['_id','vectorString'])\n",
    "for i in idxPairs:\n",
    "    if i[0][:-3] in dfV3_keys and i[0][:-3] != 'vectorString':\n",
    "        cvssV3.append(i[0])\n",
    "        dfV3_keys_cp = dfV3_keys_cp.drop([i[0][:-3]])\n",
    "\n",
    "    \n",
    "    if i[0][:-3] in dfV2_keys and i[0][:-3] != 'vectorString':\n",
    "        cvssV2.append(i[1])\n",
    "        dfV2_keys_cp = dfV2_keys_cp.drop([i[0][:-3]])\n",
    "\n",
    "cvssV3 = dfV3_keys_cp.union(cvssV3)  \n",
    "cvssV2 = dfV2_keys_cp.union(cvssV2)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copies the main df, selecting all columns and then dropping types = object (relevant data have dtype float, int,etc), the column `acInsufInfo` and all remaining rows with NaN's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNan = df.loc[:,df.dtypes != 'object'].drop('acInsufInfo',axis=1).dropna()\n",
    "\n",
    "X_pt2 = dfNoNan[cvssV2.drop('acInsufInfo')]\n",
    "y_pt2 = dfNoNan['scope'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you build the training / validation / testing dataset?\n",
    "\n",
    "Take a random subset of the data and split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt2_train, X_pt2_test, y_pt2_train, y_pt2_test = train_test_split(X_pt2, y_pt2, test_size= .33) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intended to scale data however all iterations tried made little difference on this dataset, possibly due to high n of categorical inputs and low variance in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = StandardScaler()\n",
    "#X_pt2_train = sc.fit_transform(X_pt2_train)\n",
    "#X_pt2_test = sc.transform(X_pt2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percentage of total scope count that is = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.scope.value_counts()[1]/df.scope.value_counts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which evaluation metrics would you use?\n",
    "\n",
    "a helper function which takes in a model, fits it with train values, predicts the test values and checks it against the y test values returning a dict of all results.\n",
    "\n",
    "NOTE: for compatibility with multioutput models, a custom score() function is defined later on and a value of 0 is given for a multioutput confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_info(model,X,y):\n",
    "    #takes model, set of X, set of y\n",
    "    #returns dict\n",
    "    X_train, X_test = X\n",
    "    y_train, y_test = y\n",
    "    print('~~~ fitting model')\n",
    "    f = model.fit(X_train.values, y_train.values)\n",
    "    print('~~~ predicting values')\n",
    "    ŷ = model.predict(X_test.values)\n",
    "    print('~~~ checking validity')\n",
    "    \n",
    "    try:\n",
    "        sc = f.score(X_test.values, y_test.values)\n",
    "    except:\n",
    "        sc= score(X_test.values, y_test.values)\n",
    "    \n",
    "    m = mean_absolute_error(y_test.values, ŷ)\n",
    "    c = confusion_matrix(y_test.values, ŷ) if len(y_train.shape) == 1 else 0\n",
    "\n",
    "    dict_ = {'model': f, 'score' : sc, 'prediction': ŷ, 'MAE' : m, 'Confusion Matrix': c}\n",
    "    return dict_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "builds a dict of all the model dicts with model name as key and an index corresponding to model list location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_dict(mod_list,X,Y):\n",
    "    #takes a list of models,set of X, set of y\n",
    "    #returns dict\n",
    "    mod_dict ={}\n",
    "    idx = 0\n",
    "    for i in mod_list: \n",
    "        mod_type = i.__str__()\n",
    "        print(f\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~\\nWorking on {mod_type}\")\n",
    "        if len(Y[0].shape) > 1:\n",
    "            mod_info = model_info(MultiOutputRegressor(i),X,Y)\n",
    "        mod_info = model_info(i,X,Y)\n",
    "        mod_dict[mod_type] =  (idx ,mod_info)\n",
    "        idx+=1\n",
    "        pprint(mod_info)\n",
    "        \n",
    "    return mod_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What simple model might be used for this problem? Could this be improved upon with a more complex solution?\n",
    "\n",
    "The Random Forest Classifier is repeatedly the best performer here. Ensemble Learning would be a good canditate here for improving on the given results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RFC = RandomForestClassifier()\n",
    "model_SVC = svm.SVC()\n",
    "model_MLP = MLPClassifier()\n",
    "model_LR =  LogisticRegression()\n",
    "model_LDA =  LinearDiscriminantAnalysis()\n",
    "model_KNN =  KNeighborsClassifier()\n",
    "model_CART =  DecisionTreeClassifier()\n",
    "model_NB =  GaussianNB()\n",
    "\n",
    "mod_list_pt2 = [\n",
    "    \n",
    "    #model_SVC,#slow\n",
    "    model_MLP,\n",
    "    model_LR,\n",
    "    model_LDA,\n",
    "    model_KNN,\n",
    "    model_CART,\n",
    "    model_RFC,\n",
    "    model_NB\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "mod_dict_pt2 = model_dict(mod_list_pt2,[X_pt2_train,X_pt2_test], [y_pt2_train,y_pt2_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ŷ dict built from the model dict for easier access to ŷ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ŷ_test_dict(mod_list):\n",
    "    ŷtest_dict = {}\n",
    "    for i in mod_list:\n",
    "        #print(i)\n",
    "        ŷ = pd.DataFrame(mod_dict_pt2[str(i)][1]['prediction'])\n",
    "        ŷtest_dict[i.__str__()] =  ŷ\n",
    "    return ŷtest_dict\n",
    "ŷ_pt2_test_vals = ŷ_test_dict(mod_list_pt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check scope count = 1 for each model to compare against the value given above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ŷ_pt2_test_vals:\n",
    "    print(ŷ_pt2_test_vals[i][0][1]/ŷ_pt2_test_vals[i][0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model_chk(mod_dict):\n",
    "    #takes model dict \n",
    "    #returns tuple of index and model score \n",
    "    best_mod= ('',0)\n",
    "    for i in mod_dict:\n",
    "        presc = mod_dict[i][1]['score']\n",
    "        sc = np.average(presc) #if isinstance(presc,(int,float)) else np.average(presc)\n",
    "        if sc > best_mod[1]:\n",
    "            best_mod = mod_dict[i][0], mod_dict[i][1]['score']\n",
    "\n",
    "    print(best_mod)\n",
    "    return best_mod\n",
    "work_model_pt2 = mod_list_pt2[best_model_chk(mod_dict_pt2)[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selects data where scope = nan\n",
    "\n",
    "selects all columns with v2 data except for `acInsufInfo` and drops rows with nan\n",
    "\n",
    "predicts ŷ based on the best model picked from prev function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_pt2 = df[df['scope'].isna()]\n",
    "\n",
    "Xnew_pt2 = df_y_pt2[cvssV2.drop('acInsufInfo')].dropna()\n",
    "\n",
    "ŷ_pt2 = work_model_pt2.predict(Xnew_pt2.values)\n",
    "\n",
    "pd.DataFrame(ŷ_pt2).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterates through all models in the list and produces a dict of results with model name as key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_pt2_vals = {}\n",
    "for i in mod_list_pt2:\n",
    "    ŷ_pt2 = pd.DataFrame(i.predict(Xnew_pt2.values))\n",
    "    ŷ_pt2_vals[i.__str__()] = ŷ_pt2.value_counts(), ŷ_pt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ŷ_pt2_vals:\n",
    "    print(ŷ_pt2_vals[i][0][1]/ŷ_pt2_vals[i][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 3\n",
    "Predict the CVSSv3 Confidentiality, Integrity and Availability metrics for CVEs without a CVSSv3 vector.\n",
    "- What type of learning problem is this? What is the target? \n",
    "\n",
    "\n",
    "This is a multi output supervised regression problem with 3 targets, `confidentialityImpact_V3`, `integrityImpact_V3`, `availabilityImpact_V3`\n",
    "\n",
    "each target is calculated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_RFR = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list_pt3 = [\n",
    "    \n",
    "    model_RFR,\n",
    "    #model_KNN,#slow\n",
    "    model_CART,\n",
    "    model_RFC,\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNan = df.loc[:,df.dtypes != 'object'].drop('acInsufInfo',axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.Index(['confidentialityImpact_V3', 'integrityImpact_V3', 'availabilityImpact_V3'])\n",
    "X = dfNoNan[cvssV2.drop('acInsufInfo')]\n",
    "y = dfNoNan[response].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you build the training / validation / testing dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt3_train, X_pt3_test, y_pt3_train, y_pt3_test = train_test_split(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Which evaluation metrics would you use?\n",
    "\n",
    "this is a really simple metric based on the sklearn `model.score()` function which gets  1 - total correct divided by total n. My implementation will take multioutput and returns a vector of len(ŷ). Does NOT work for single target ŷ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_test, ŷ): \n",
    "    #takes y known , ŷ\n",
    "    #returns arr with len(ŷ)\n",
    "\n",
    "    arr_1 = y_test\n",
    "    arr_2 = ŷ\n",
    "\n",
    "    if len(arr_1)!=len(arr_2):\n",
    "        print(len(arr_1), len(arr_2))\n",
    "        print('!!! NOT the same length !!!')\n",
    "        return\n",
    "\n",
    "    shape = arr_2.shape\n",
    " \n",
    "    truth_d = {True:[0]*shape[1], False:[0]*shape[1]}\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            truth_d[arr_1[i][j] == arr_2[i][j]][j] += 1\n",
    "\n",
    "    return [1 - truth_d[True][i]/ shape[0] for i in range(shape[1])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What simple model might be used for this problem? Could this be improved upon with a more complex solution?\n",
    "\n",
    "\n",
    "Here the Multioutput Regressor acts as a wrapper around estimators. This allows for direct regression of each individual estimator. as can be seen from the Random Forest Regressor prediction the output is continous\n",
    "\n",
    "a better approach would be to use a chained regressor which would chain each regression together in a conditinal manner i.e;   y1  , y2|ŷ1 ,  y3|( ŷ1 & ŷ2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict_pt3 = model_dict(mod_list_pt3,[X_pt3_train,X_pt3_test], [y_pt3_train,y_pt3_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_pt3_test_vals = ŷ_test_dict(mod_dict_pt3)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
