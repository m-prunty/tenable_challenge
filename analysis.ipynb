{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Data is [fetched from the Api](api_.py) socket provided, https://services.nvd.nist.gov/rest/json/cves/1.0/. \n",
    "\n",
    "A local [MongoDB is established](mongo_.py) and the data is upserted. \n",
    "\n",
    "With [mongo queries](pddf.py) the data is unraveled, split into two, for CVSSv2 and CVSSv3. It is then returned as pandas' dataframes.\n",
    "\n",
    "The `Run()` class will perform all this with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from run import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish and Fill MongoDB\n",
    "Connects to the API and builds succesive queries by default starting in 2014 waiting 1s between each query.\n",
    "\n",
    "Upserts to a local MongoDB established with default settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#NOTE: will automatically start downloading and\n",
    "# try to connect to a default mongoDB client\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "#Run(collection='t').fill_mongo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Explore the data to get a better understanding of the content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Pandas' dataframes\n",
    "Two df's returned for CVSSv2 and v3 which are merged together using an outer join\n",
    "\n",
    "- Create a Data frame with one row per CVE id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = Run(collection= 't').fill_df()\n",
    "dfV2,dfV3 = dfs.dfV2,dfs.dfV3\n",
    "try:\n",
    "    df = pd.merge( dfV3, dfV2, 'outer', '_id',suffixes=['_V3', '_V2'])\n",
    "except:\n",
    "    print('''ERROR: Possibly no MongoDB loaded\\nCreating df from backupDB.csv''')\n",
    "    pd.read_csv('backupDB.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many CVEs have CVSSv3 metrics versus only CVSSv2 metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n of CVE's = 115262\n",
      "with CVSSv3 = 100248\n",
      "with CVSSv2 = 114569\n",
      "with just CVSSv2 = 14420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Total n of CVE's = {len(df)}\n",
    "with CVSSv3 = {len(df.vectorString_V3.dropna())}\n",
    "with CVSSv2 = {len(df.vectorString_V2.dropna())}\n",
    "with just CVSSv2 = {len((df[df['vectorString_V3'].isnull()])['vectorString_V2'].dropna())}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for unique values in each column to determine if categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan '3.1' '3.0'] version_V3\n",
      "[nan 'NETWORK' 'LOCAL' 'PHYSICAL' 'ADJACENT_NETWORK'] attackVector\n",
      "[nan 'LOW' 'HIGH'] attackComplexity\n",
      "[nan 'NONE' 'LOW' 'HIGH'] privilegesRequired\n",
      "[nan 'NONE' 'REQUIRED'] userInteraction\n",
      "[nan 'UNCHANGED' 'CHANGED'] scope\n",
      "[nan 'HIGH' 'LOW' 'NONE'] confidentialityImpact_V3\n",
      "[nan 'HIGH' 'LOW' 'NONE'] integrityImpact_V3\n",
      "[nan 'HIGH' 'NONE' 'LOW'] availabilityImpact_V3\n",
      "[nan 'CRITICAL' 'MEDIUM' 'HIGH' 'LOW'] baseSeverity\n",
      "['2.0' nan] version_V2\n",
      "['NETWORK' 'LOCAL' 'ADJACENT_NETWORK' nan] accessVector\n",
      "['LOW' 'MEDIUM' 'HIGH' nan] accessComplexity\n",
      "['NONE' 'SINGLE' 'MULTIPLE' nan] authentication\n",
      "['NONE' 'PARTIAL' 'COMPLETE' nan] confidentialityImpact_V2\n",
      "['PARTIAL' 'NONE' 'COMPLETE' nan] integrityImpact_V2\n",
      "['NONE' 'PARTIAL' 'COMPLETE' nan] availabilityImpact_V2\n",
      "['MEDIUM' 'HIGH' 'LOW' nan] severity\n",
      "[False True nan] obtainAllPrivilege\n",
      "[False True nan] obtainUserPrivilege\n",
      "[False True nan] obtainOtherPrivilege\n",
      "[False True nan] userInteractionRequired\n",
      "[nan False True] acInsufInfo\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    if df[i].dtype == object and len(df[i].unique()) <10:\n",
    "        print (df[i].unique(), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attackVector` and `accessVector` are nominal categorical and need dummy variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δ_list = [['attackVector','AV_3'], ['accessVector', 'AV_2']]\n",
    "dum_add = lambda ele: pd.get_dummies(df[ele[0]],prefix=ele[1])\n",
    "\n",
    "frames = [dum_add(i) for i in Δ_list]\n",
    "\n",
    "df = df.drop([i[0] for i in Δ_list], axis=1)\n",
    "\n",
    "frames.append(df)\n",
    "\n",
    "df = pd.concat(frames,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the other columns are ordinal categorical(e.g. `LOW`, `MEDIUM`, `HIGH` ) or boolean and can be filled with a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = {'NONE': 0, 'LOW':1, 'MEDIUM':2,'HIGH':3, 'CRITICAL':4,\n",
    "'PARTIAL': 1, 'COMPLETE':2,\n",
    "'SINGLE' :1, 'MULTIPLE' :2,\n",
    "'UNCHANGED':0 ,'CHANGED':1,\n",
    "'REQUIRED':1,\n",
    "False:0, True:1\n",
    "}\n",
    "df.replace(cat_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating general index of all keys for manipulating columns and adds the new dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfV2_keys,dfV3_keys = dfs.dfV2.keys(),dfs.dfV3.keys()\n",
    "\n",
    "\n",
    "transform_idx = df.T.index\n",
    "\n",
    "for i in transform_idx:\n",
    "    #print(i[:4])\n",
    "    if i[:4] == 'AV_3':\n",
    "        dfV3_keys = dfV3_keys.append(pd.Index([i]))\n",
    "    if i[:4] == 'AV_2':\n",
    "        dfV2_keys = dfV2_keys.append(pd.Index([i]))\n",
    "\n",
    "dfV3_keys = dfV3_keys.drop('attackVector')\n",
    "dfV2_keys = dfV2_keys.drop('accessVector')\n",
    "\n",
    "setDf = dfV3_keys.union(dfV2_keys)#set(dfV3_keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating an index of all columns that appear in both CVSSv2 and v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxPairs = []\n",
    "\n",
    "for i in setDf: \n",
    "    substr_i = transform_idx[transform_idx.str.startswith(i+'_')]\n",
    "    if len(substr_i) >0:\n",
    "        idxPairs.append(substr_i)\n",
    "    \n",
    "\n",
    "#idxPairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df.filter(regex='version*').columns\n",
    "df[f] = df[f].astype(float)\n",
    "df = df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both CVSSv2 and CVSSv3 have the same set of impact metrics, i.e. Confidentiality, Integrity and Availability, however their values are slightly different. For example, CVSSv2 uses complete (C) to represent the highest level of impact, but CVSSv3 uses high (H) instead. Is it possible to directly map from CVSSv2 impact metric values to CVSSv3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8024715511898207\n",
      "0.7847626572053179\n",
      "0.8352650731843352\n"
     ]
    }
   ],
   "source": [
    "for i in ['confidentialityImpact','integrityImpact','availabilityImpact']:\n",
    "    print(df.corr()[i+'_V2'][i+'_V3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the correleation matrix between versions of the suggested metrics is not = 1, it would NOT be a good idea to directly map them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "predict the CVSSv3 Scope metric for CVEs without a CVSSv3 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What type of learning problem is this? What is the target? \n",
    "\n",
    "This is a supervised classification problem with a single variable `scope` as the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis  import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selects and combines the metrics of interest from previous indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cvssV3 = []\n",
    "cvssV2 = []\n",
    "dfV3_keys_cp = dfV3_keys.drop(['_id','vectorString'])\n",
    "dfV2_keys_cp = dfV2_keys.drop(['_id','vectorString'])\n",
    "for i in idxPairs:\n",
    "    if i[0][:-3] in dfV3_keys and i[0][:-3] != 'vectorString':\n",
    "        cvssV3.append(i[0])\n",
    "        dfV3_keys_cp = dfV3_keys_cp.drop([i[0][:-3]])\n",
    "\n",
    "    \n",
    "    if i[0][:-3] in dfV2_keys and i[0][:-3] != 'vectorString':\n",
    "        cvssV2.append(i[1])\n",
    "        dfV2_keys_cp = dfV2_keys_cp.drop([i[0][:-3]])\n",
    "\n",
    "cvssV3 = dfV3_keys_cp.union(cvssV3)  \n",
    "cvssV2 = dfV2_keys_cp.union(cvssV2)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copies the main df, selecting all columns and then dropping types = object (relevant data have dtype float, int,etc), the column `acInsufInfo` and all remaining rows with NaN's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNan = df.loc[:,df.dtypes != 'object'].drop('acInsufInfo',axis=1).dropna()\n",
    "\n",
    "X_pt2 = dfNoNan[cvssV2.drop('acInsufInfo')]\n",
    "y_pt2 = dfNoNan['scope'].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you build the training / validation / testing dataset?\n",
    "\n",
    "Take a random subset of the data and split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt2_train, X_pt2_test, y_pt2_train, y_pt2_test = train_test_split(X_pt2, y_pt2, test_size= .33) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "intended to scale data however all iterations tried made little difference on this dataset, possibly due to high n of categorical inputs and low variance in numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = StandardScaler()\n",
    "#X_pt2_train = sc.fit_transform(X_pt2_train)\n",
    "#X_pt2_test = sc.transform(X_pt2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percentage of total scope count that is = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20283647096936755"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.scope.value_counts()[1]/df.scope.value_counts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which evaluation metrics would you use?\n",
    "\n",
    "a helper function which takes in a model, fits it with train values, predicts the test values and checks it against the y test values returning a dict of all results.\n",
    "\n",
    "NOTE: for compatibility with multioutput models, a custom score() function is defined later on and a value of 0 is given for a multioutput confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_info(model,X,y):\n",
    "    #takes model, set of X, set of y\n",
    "    #returns dict\n",
    "    X_train, X_test = X\n",
    "    y_train, y_test = y\n",
    "    print('~~~ fitting model')\n",
    "    f = model.fit(X_train.values, y_train.values)\n",
    "    print('~~~ predicting values')\n",
    "    ŷ = model.predict(X_test.values)\n",
    "    print('~~~ checking validity')\n",
    "    \n",
    "    try:\n",
    "        sc = f.score(X_test.values, y_test.values)\n",
    "    except:\n",
    "        sc= score(X_test.values, y_test.values)\n",
    "    \n",
    "    m = mean_absolute_error(y_test.values, ŷ)\n",
    "    c = confusion_matrix(y_test.values, ŷ) if len(y_train.shape) == 1 else 0\n",
    "\n",
    "    dict_ = {'model': f, 'score' : sc, 'prediction': ŷ, 'MAE' : m, 'Confusion Matrix': c}\n",
    "    return dict_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "builds a dict of all the model dicts with model name as key and an index corresponding to model list location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_dict(mod_list,X,Y):\n",
    "    #takes a list of models,set of X, set of y\n",
    "    #returns dict\n",
    "    mod_dict ={}\n",
    "    idx = 0\n",
    "    for i in mod_list: \n",
    "        mod_type = i.__str__()\n",
    "        print(f\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~\\nWorking on {mod_type}\")\n",
    "        if len(Y[0].shape) > 1:\n",
    "            mod_info = model_info(MultiOutputRegressor(i),X,Y)\n",
    "        mod_info = model_info(i,X,Y)\n",
    "        mod_dict[mod_type] =  (idx ,mod_info)\n",
    "        idx+=1\n",
    "        pprint(mod_info)\n",
    "        \n",
    "    return mod_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What simple model might be used for this problem? Could this be improved upon with a more complex solution?\n",
    "\n",
    "The Random Forest Classifier is repeatedly the best performer here. Ensemble Learning would be a good canditate here for improving on the given results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on MLPClassifier()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[26610,   624],\n",
      "       [ 1176,  4347]]),\n",
      " 'MAE': 0.054950087004304426,\n",
      " 'model': MLPClassifier(),\n",
      " 'prediction': array([0, 0, 0, ..., 0, 0, 0]),\n",
      " 'score': 0.9450499129956956}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on LogisticRegression(max_iter=500)\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[26457,   777],\n",
      "       [ 1524,  3999]]),\n",
      " 'MAE': 0.07024452788716916,\n",
      " 'model': LogisticRegression(max_iter=500),\n",
      " 'prediction': array([0, 0, 0, ..., 0, 0, 0]),\n",
      " 'score': 0.9297554721128308}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on LinearDiscriminantAnalysis()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[26032,  1202],\n",
      "       [ 1110,  4413]]),\n",
      " 'MAE': 0.07058033397441768,\n",
      " 'model': LinearDiscriminantAnalysis(),\n",
      " 'prediction': array([0, 0, 0, ..., 0, 0, 0]),\n",
      " 'score': 0.9294196660255823}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on KNeighborsClassifier()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[26444,   790],\n",
      "       [ 1157,  4366]]),\n",
      " 'MAE': 0.059437677442989285,\n",
      " 'model': KNeighborsClassifier(),\n",
      " 'prediction': array([0, 0, 0, ..., 0, 0, 0]),\n",
      " 'score': 0.9405623225570107}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on DecisionTreeClassifier()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[26578,   656],\n",
      "       [ 1139,  4384]]),\n",
      " 'MAE': 0.05479744787373691,\n",
      " 'model': DecisionTreeClassifier(),\n",
      " 'prediction': array([0, 0, 0, ..., 0, 0, 0]),\n",
      " 'score': 0.9452025521262631}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on RandomForestClassifier()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[26582,   652],\n",
      "       [ 1140,  4383]]),\n",
      " 'MAE': 0.0547058643953964,\n",
      " 'model': RandomForestClassifier(),\n",
      " 'prediction': array([0, 0, 0, ..., 0, 0, 0]),\n",
      " 'score': 0.9452941356046036}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on GaussianNB()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': array([[17844,  9390],\n",
      "       [  828,  4695]]),\n",
      " 'MAE': 0.3119333272277681,\n",
      " 'model': GaussianNB(),\n",
      " 'prediction': array([1, 0, 1, ..., 0, 1, 1]),\n",
      " 'score': 0.6880666727722319}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_SVC = svm.SVC()\n",
    "model_MLP = MLPClassifier()\n",
    "model_LR =  LogisticRegression(max_iter= 500)\n",
    "model_LDA =  LinearDiscriminantAnalysis()\n",
    "model_KNN =  KNeighborsClassifier()\n",
    "model_CART =  DecisionTreeClassifier()\n",
    "model_RFC = RandomForestClassifier()\n",
    "model_NB =  GaussianNB()\n",
    "\n",
    "mod_list_pt2 = [\n",
    "    \n",
    "    #model_SVC,#slow\n",
    "    model_MLP,\n",
    "    model_LR,\n",
    "    model_LDA,\n",
    "    model_KNN,\n",
    "    model_CART,\n",
    "    model_RFC,\n",
    "    model_NB\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "mod_dict_pt2 = model_dict(mod_list_pt2,[X_pt2_train,X_pt2_test], [y_pt2_train,y_pt2_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ŷ dict built from the model dict for easier access to ŷ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ŷ_test_dict(mod_dict):\n",
    "    ŷtest_dict = {}\n",
    "    for i in mod_dict:\n",
    "        ŷ = pd.DataFrame(mod_dict[i][1]['prediction'])\n",
    "        ŷtest_dict[i.__str__()] =  ŷ\n",
    "    return ŷtest_dict\n",
    "ŷ_pt2_test_vals = ŷ_test_dict(mod_dict_pt2)\n",
    "\n",
    "#ŷ_pt2_test_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0.9452025521262631)\n"
     ]
    }
   ],
   "source": [
    "def best_model_chk(mod_dict):\n",
    "    #takes model dict \n",
    "    #returns tuple of index and model score \n",
    "    best_mod= ('',0)\n",
    "    for i in mod_dict:\n",
    "        presc = mod_dict[i][1]['score']\n",
    "        sc = np.average(presc) #if isinstance(presc,(int,float)) else np.average(presc)\n",
    "        if sc > best_mod[1]:\n",
    "            best_mod = mod_dict[i][0], mod_dict[i][1]['score']\n",
    "\n",
    "    print(best_mod)\n",
    "    return best_mod\n",
    "work_model_pt2 = mod_list_pt2[best_model_chk(mod_dict_pt2)[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selects data where scope = nan\n",
    "\n",
    "selects all columns with v2 data except for `acInsufInfo` and drops rows with nan\n",
    "\n",
    "predicts ŷ based on the best model picked from prev function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scope,)\n",
       "0           12160\n",
       "1            2123\n",
       "dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_pt2 = df[df['scope'].isna()]\n",
    "\n",
    "Xnew_pt2 = df_y_pt2[cvssV2.drop('acInsufInfo')].dropna()\n",
    "\n",
    "index_pt2 = Xnew_pt2.index\n",
    "\n",
    "ŷ_pt2 = pd.DataFrame(work_model_pt2.predict(Xnew_pt2.values), index = index_pt2, columns=[['scope']])\n",
    "\n",
    "ŷ_pt2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_pt2_vals = {}\n",
    "for i in mod_list_pt2:\n",
    "    temp = pd.DataFrame(i.predict(Xnew_pt2.values))\n",
    "    ŷ_pt2_vals[i.__str__()] = ŷ_pt2.value_counts(),temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17555555555555555\n",
      "0.16757949807896674\n",
      "0.18590169378943872\n",
      "0.17759089784813256\n",
      "0.17574909450115245\n",
      "0.17420256494574154\n",
      "0.5985450475657527\n"
     ]
    }
   ],
   "source": [
    "for i in ŷ_pt2_vals:\n",
    "    print(ŷ_pt2_vals[i][0][1]/ŷ_pt2_vals[i][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted `scope` df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>scope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15806</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15807</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15808</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15809</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64117</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14283 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scope\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "...     ...\n",
       "15806     0\n",
       "15807     0\n",
       "15808     0\n",
       "15809     0\n",
       "64117     1\n",
       "\n",
       "[14283 rows x 1 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷ_pt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 3\n",
    "Predict the CVSSv3 Confidentiality, Integrity and Availability metrics for CVEs without a CVSSv3 vector.\n",
    "- What type of learning problem is this? What is the target? \n",
    "\n",
    "\n",
    "This is a multi output supervised regression problem with 3 targets, `confidentialityImpact_V3`, `integrityImpact_V3`, `availabilityImpact_V3`\n",
    "\n",
    "each target is calculated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_RFR = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list_pt3 = [\n",
    "    \n",
    "    model_RFR,\n",
    "    #model_KNN,#slow\n",
    "    model_CART,\n",
    "    model_RFC,\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNan = df.loc[:,df.dtypes != 'object'].drop('acInsufInfo',axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.Index(['confidentialityImpact_V3', 'integrityImpact_V3', 'availabilityImpact_V3'])\n",
    "X = dfNoNan[cvssV2.drop('acInsufInfo')]\n",
    "y = dfNoNan[response].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you build the training / validation / testing dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt3_train, X_pt3_test, y_pt3_train, y_pt3_test = train_test_split(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Which evaluation metrics would you use?\n",
    "\n",
    "this is a really simple metric based on the sklearn `model.score()` function which gets  1 - total correct divided by total n. My implementation will take multioutput and returns a vector of len(ŷ). Does NOT work for single target ŷ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_test, ŷ): \n",
    "    #takes y known , ŷ\n",
    "    #returns arr with len(ŷ)\n",
    "\n",
    "    arr_1 = y_test\n",
    "    arr_2 = ŷ\n",
    "\n",
    "    if len(arr_1)!=len(arr_2):\n",
    "        print(len(arr_1), len(arr_2))\n",
    "        print('!!! NOT the same length !!!')\n",
    "        return\n",
    "\n",
    "    shape = arr_2.shape\n",
    " \n",
    "    truth_d = {True:[0]*shape[1], False:[0]*shape[1]}\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            truth_d[arr_1[i][j] == arr_2[i][j]][j] += 1\n",
    "\n",
    "    return [1 - truth_d[True][i]/ shape[0] for i in range(shape[1])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What simple model might be used for this problem? Could this be improved upon with a more complex solution?\n",
    "\n",
    "\n",
    "Here the Multioutput Regressor acts as a wrapper around estimators. This allows for direct regression of each individual estimator. as can be seen from the Random Forest Regressor prediction the output is continous\n",
    "\n",
    "a better approach would be to use a chained regressor which would chain each regression together in a conditinal manner i.e;   y1  , y2|ŷ1 ,  y3|( ŷ1 & ŷ2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on RandomForestRegressor()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': 0,\n",
      " 'MAE': 0.21795856159953483,\n",
      " 'model': RandomForestRegressor(),\n",
      " 'prediction': array([[2.78455130e-01, 2.18375109e+00, 2.38857444e-02],\n",
      "       [2.99892037e+00, 3.00000000e+00, 2.99783245e+00],\n",
      "       [9.94455451e-01, 1.02390032e+00, 8.67838768e-03],\n",
      "       ...,\n",
      "       [8.20706054e-01, 1.22672856e+00, 8.02149324e-03],\n",
      "       [0.00000000e+00, 2.35445386e-03, 3.00000000e+00],\n",
      "       [2.88855498e+00, 2.90345594e+00, 2.88284513e+00]]),\n",
      " 'score': 0.8655463148907462}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on DecisionTreeClassifier()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': 0,\n",
      " 'MAE': 0.14012465076294864,\n",
      " 'model': DecisionTreeClassifier(),\n",
      " 'prediction': array([[0, 3, 0],\n",
      "       [3, 3, 3],\n",
      "       [1, 1, 0],\n",
      "       ...,\n",
      "       [1, 1, 0],\n",
      "       [0, 0, 3],\n",
      "       [3, 3, 3]]),\n",
      " 'score': [0.7883623468729852, 0.7311009026434558, 0.934115087040619]}\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Working on RandomForestClassifier()\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "~~~ fitting model\n",
      "~~~ predicting values\n",
      "~~~ checking validity\n",
      "{'Confusion Matrix': 0,\n",
      " 'MAE': 0.1401649473457984,\n",
      " 'model': RandomForestClassifier(),\n",
      " 'prediction': array([[0, 3, 0],\n",
      "       [3, 3, 3],\n",
      "       [1, 1, 0],\n",
      "       ...,\n",
      "       [1, 1, 0],\n",
      "       [0, 0, 3],\n",
      "       [3, 3, 3]]),\n",
      " 'score': [0.7883623468729852, 0.7311009026434558, 0.934115087040619]}\n"
     ]
    }
   ],
   "source": [
    "mod_dict_pt3 = model_dict(mod_list_pt3,[X_pt3_train,X_pt3_test], [y_pt3_train,y_pt3_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_pt3_test_vals = ŷ_test_dict(mod_dict_pt3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.8655463148907462)\n"
     ]
    }
   ],
   "source": [
    "work_model_pt3 = mod_list_pt3[best_model_chk(mod_dict_pt3)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan here is as above select all response variables from the df that = NaN\n",
    "\n",
    "selects all columns with v2 data except for `acInsufInfo` and drop rows with nan\n",
    "\n",
    "predict ŷ based on the best model picked from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_y_pt3 = df[ df['vectorString_V3' ].isna()]\n",
    "\n",
    "Xnew_pt3 = df_y_pt3[cvssV2.drop('acInsufInfo')].dropna()\n",
    "\n",
    "\n",
    "index_pt3 = Xnew_pt3.index\n",
    "\n",
    "\n",
    "ŷ_pt3 = pd.DataFrame(work_model_pt3.predict(Xnew_pt3.values),index = index_pt3,columns = response).round().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidentialityImpact_V3</th>\n",
       "      <th>integrityImpact_V3</th>\n",
       "      <th>availabilityImpact_V3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15806</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15807</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15808</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15809</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64117</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14283 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       confidentialityImpact_V3  integrityImpact_V3  availabilityImpact_V3\n",
       "0                             0                   2                      0\n",
       "1                             3                   3                      3\n",
       "2                             3                   3                      3\n",
       "3                             3                   3                      3\n",
       "4                             3                   3                      3\n",
       "...                         ...                 ...                    ...\n",
       "15806                         3                   0                      0\n",
       "15807                         3                   0                      0\n",
       "15808                         3                   3                      2\n",
       "15809                         3                   0                      0\n",
       "64117                         1                   1                      0\n",
       "\n",
       "[14283 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ŷ_pt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
